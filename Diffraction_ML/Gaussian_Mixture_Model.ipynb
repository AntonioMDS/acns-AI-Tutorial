{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CAMM-UTK/acns-AI-tutorial/blob/main/Diffraction_ML/Gaussian_Mixture_Model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For questions/comments, please contact [Krishnanand Mallayya](https://www.linkedin.com/in/krishnanandmallayya/)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4NJdlBLujhJ"
   },
   "source": [
    "# 1. Unsupervised learning (GMM) for diffraction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxfxJ5YIujhJ"
   },
   "source": [
    "In this notebook, we will explore unsupervised machine learning through clustering. Clustering allows us to discover inherent structures or groupings within data without prior knowledge or labels.\n",
    "\n",
    "We will introduce Gaussian mixture model (GMM) clustering with the goal of identifying clusters of distinct parametric dependencies (such as temperature dependence) in a dataset.\n",
    "\n",
    "\n",
    "The dataset in this notebook is derived from intensities of X-ray diffraction evolving as a function of temperature. The data is already preprocessed and serves as an illustration of how to implement GMM clustering using scikit-learn.  \n",
    "\n",
    "In the second notebook ([XTEC_with_GMM](https://github.com/CAMM-UTK/acns-AI-tutorial/blob/main/Diffraction_ML/XTEC_with_GMM.ipynb)), we will explore the actual X-ray diffraction temperature series data. There, we will demonstrate the preprocessing, clustering, and visualization of clustered diffraction patterns using the python package: X-ray Diffraction Temperature Clustering (X-TEC). The same approach can be extended to analyze various other types of data evolving under a parameter, such as time, B-field, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQnhZ404ujhK"
   },
   "source": [
    "### Imports and set-ups for running in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8__ZRgL7ujhK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import time\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_k_aV16ujhK"
   },
   "source": [
    "## 1.1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ew1yUd3ZujhL",
    "outputId": "eb9efd6a-1dee-4eb8-f335-06fb2d1f1810"
   },
   "outputs": [],
   "source": [
    "# Download the data\n",
    "!wget -O Signal_vs_T.csv \"https://www.dropbox.com/scl/fi/9pqmngxzjkmcwl1wwm8jg/Signal_vs_T.csv?rlkey=q9curpt809bj5nupvdelg2600\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jak2fQauujhL"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Signal_vs_T.csv', sep=' ', index_col=0,header=0)\n",
    "df.columns.name='Temperature (K)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "PYPjt-x7ujhL",
    "outputId": "1e3efd68-dc9f-43f6-b2cc-9f3fc5f9134f"
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mJ9NEL2ujhL"
   },
   "source": [
    "The data has 56868 different signals (which are actually the rescaled intensities of XRD, which you will see in the second notebook), at 19 different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plQUefDwujhL"
   },
   "outputs": [],
   "source": [
    "Temperature = df.columns.values.astype('float')\n",
    "Signals= df.values.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgPLubePujhL"
   },
   "source": [
    "## 1.2 Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-HAp-1UujhL"
   },
   "source": [
    "Lets plot a few of these signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "dvB_7eTWujhM",
    "outputId": "165ebb18-6db4-443b-d4cd-51c03ceb933a"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "\n",
    "# plotting the first 5000 signals\n",
    "for i in range(0,5000):\n",
    "    plt.plot(Temperature,Signals[i])\n",
    "\n",
    "plt.xlabel('$T$ (K) ',size=20)\n",
    "plt.ylabel(r'Signals',size=20)\n",
    "plt.xticks(np.arange(30, 310, 50), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim([-3,2.5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDDL-nYNujhM"
   },
   "source": [
    "We can see some groups of distinct temperature dependencies. To visually illustrate how this translates to clustering with Gaussian Mixture model, let us first simplify the problem.  \n",
    "\n",
    "\n",
    "Each signal in this dataset is a 19-dimensional vector.  Let us visualise them in 2D, by taking a 2D cross-section of the 19 dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGJtCh-LujhM"
   },
   "source": [
    "### Let's select two temperatures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQyUKifcujhM",
    "outputId": "29fa0999-7d11-4da7-aabe-1a79c5baa1fd"
   },
   "outputs": [],
   "source": [
    "a=1\n",
    "b=4\n",
    "\n",
    "Ta = Temperature[a]\n",
    "Tb = Temperature[b]\n",
    "\n",
    "print(Ta,Tb)\n",
    "\n",
    "\n",
    "Sa=Signals[:,a]\n",
    "Sb=Signals[:,b]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-E9pQlXujhM"
   },
   "source": [
    "### Plot this 2D cross-section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "SGCBZqcbujhM",
    "outputId": "6dd44bd1-76e7-4adb-a977-016effdf5d03"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.scatter(Sa,Sb,s=3,alpha=0.2)\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([-0.8,1.5])\n",
    "plt.xticks(np.arange(-1.0,2.5,1),fontsize=15);\n",
    "plt.yticks(np.arange(-1.0,1.5,1),fontsize=15);\n",
    "\n",
    "plt.xlabel(f'Signal($T$ ={Ta}K)',size=20);\n",
    "plt.ylabel(f'Signal($T$ ={Tb}K)',size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hsVG_a4ujhM"
   },
   "source": [
    "However, manually inspecting 2D cross-sections is not practical for higher-dimensional data. Instead, we can use the Gaussian Mixture Models to identify clusters in the original 19-dimensional space, and identify the underlying structure in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqquDHFhujhM"
   },
   "source": [
    "## 1.3 Gaussian Mixture Model\n",
    "\n",
    "\n",
    "GMM is a probabilistic model that assumes the data is generated from a mixture of a finite number of Gaussian distributions. Each Gaussian distribution represents a cluster, and the goal is to find the parameters of these distributions that best fit the data.\n",
    "\n",
    "To apply GMM clustering to our 19-dimensional data, we can use the scikit-learn library in Python.\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFApeTOaujhM"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nL7u-quWujhM",
    "outputId": "e9406f0d-484f-4d20-e96f-dce8c590d802"
   },
   "outputs": [],
   "source": [
    "print('(num of data, num of features) = ', Signals.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqdFNPZyujhM"
   },
   "source": [
    "## 1.4 Specify number of clusters\n",
    "\n",
    "Choosing the number of clusters is not always obvious, and requires some experimentation. Setting n_clusters too large can lead to overfitting, where the clustering algorithm fits noise or other irrelevant and overly specific clusters. It can also result in fragmented clusters that are redundant and only capture minor variations. On the other hand, if it is set too small, we will miss physically important clusters. Try these different cases!\n",
    "\n",
    "\n",
    "\n",
    "Let's start with 4 clusters. Later in this notebook, we will explore a heuristic method that often comes handy in determining the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwPXmnD0ujhM"
   },
   "outputs": [],
   "source": [
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lD3oNmEujhM"
   },
   "source": [
    "## 1.5 Fit the GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtLPeE3IujhM"
   },
   "outputs": [],
   "source": [
    "# Create a Gaussian Mixture Model\n",
    "GMM = GaussianMixture(n_components=n_clusters,random_state=13) # fixing the random_state ensures results are same in all runs);\n",
    "\n",
    "# Fit the GMM\n",
    "GMM.fit(Signals);\n",
    "\n",
    "# Predict the cluster assignments (labels) for each signal\n",
    "cluster_assigns = GMM.predict(Signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eb_E6lrYujhM",
    "outputId": "28b70728-2613-4025-d186-8e69522a1851"
   },
   "outputs": [],
   "source": [
    "print(cluster_assigns)\n",
    "print(cluster_assigns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M171xl0ujhM"
   },
   "source": [
    "Cluster_assigns contains integer labels from 0 to n_clusters-1, representing the cluster assignment for each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Ov-aP4ujhN"
   },
   "source": [
    "## 1.6 Visualize the clustered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuytYNBNujhN"
   },
   "source": [
    "We will first create a list of discrete colors, one for each cluster  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IulowXGAujhN"
   },
   "outputs": [],
   "source": [
    "# Generate a set of discrete colors\n",
    "cluster_colors = cm.tab10(np.linspace(0, 1, n_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9AA26ZsujhN"
   },
   "source": [
    "Now, let's go back to the 2D cross-section again and plot the points, but this time, we will color each point according to its assigned cluster label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "3gxqQx8VujhN",
    "outputId": "35ce69fb-9814-4c38-dca3-d40c3e7d6782"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    mask = cluster_assigns == i\n",
    "    plt.scatter(Sa[mask], Sb[mask],s=3,alpha=0.2, color=cluster_colors[i], label=f'Cluster {i+1}')\n",
    "\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([-0.8,1.5])\n",
    "plt.xticks(np.arange(-1.0,2.5,1),fontsize=15);\n",
    "plt.yticks(np.arange(-1.0,1.5,1),fontsize=15);\n",
    "plt.xlabel(f'Signals ($T$ ={Ta} K)',size=20);\n",
    "plt.ylabel(f'Signals ($T$ ={Tb} K)',size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0A-hdzWujhN"
   },
   "source": [
    "We can see the different clusters distinguished by their colors. Note that the GMM is separating the structures not just based on the data distribution in this 2D cross-section, but in the entire 19 dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o699MaLQujhN"
   },
   "source": [
    "Going back to the Signal vs Temperature plot, we can now represent each signal by its cluster color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "PtqSqHA1ujhN",
    "outputId": "12aac06e-95cd-4445-c734-39d14be12d39"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "\n",
    "# plotting the first 5000 signals\n",
    "for i in range(0,5000):\n",
    "    plt.plot(Temperature,Signals[i],color=cluster_colors[cluster_assigns[i]],alpha=0.5)\n",
    "\n",
    "plt.xlabel('$T$ (K) ',size=20)\n",
    "plt.ylabel(r'Signals',size=20)\n",
    "plt.xticks(np.arange(30, 310, 50), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim([-3,2.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDtdntK_ujhN"
   },
   "source": [
    "### We now see the clusters of distinct temperature dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km06cFCqujhN"
   },
   "source": [
    "Each cluster in the GMM is described by a Gaussian with a mean and a variance in the 19 dimensional space. We can plot the distinct temperature dependencies by the cluster mean and (diagonal elements) of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRJborjHujhN"
   },
   "outputs": [],
   "source": [
    "GMM_means=GMM.means_\n",
    "GMM_cov=GMM.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh_7JxE3ujhO",
    "outputId": "70bf1e48-5c7d-4a00-bdd2-61b8bf414e13",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "\n",
    "    cluster_mean = GMM.means_[i]\n",
    "    cluster_std = np.sqrt(np.diag(GMM.covariances_[i]))  # the diagonal elements of the covariance matrix\n",
    "\n",
    "\n",
    "    plt.plot(Temperature,cluster_mean,lw=3,color=cluster_colors[i])\n",
    "    plt.gca().fill_between(Temperature,cluster_mean-cluster_std,cluster_mean+cluster_std,color=cluster_colors[i],alpha=0.5)\n",
    "\n",
    "\n",
    "plt.xlabel('$T$ (K)',size=20)\n",
    "plt.ylabel(r'Signals',size=20)\n",
    "plt.xticks(np.arange(30, 310, 50), fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim([-3,2.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seMWFtO5ujhO"
   },
   "source": [
    "### 1.7  Experimenting with the number of clusters\n",
    "\n",
    "Choosing the appropriate number of clusters require some experimentation and visualization of the results. One heuristic approach is to use the Bayesian Information Criterion (BIC) score.\n",
    "\n",
    "\n",
    "\n",
    "Let us use the BIC score to estimate the optimal number of clusters:\n",
    "\n",
    "We'll fit GMMs with varying numbers of components (clusters). For each model, we'll calculate the BIC score. We'll then plot the BIC scores against the number of components. The optimal number of clusters is typically indicated by the 'elbow' point in this plot, where the rate of BIC score improvement begins to level off.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsGZsBBAujhO"
   },
   "outputs": [],
   "source": [
    "# Define the range of n_clusters\n",
    "n_clusters_list = list(range(2, 10))\n",
    "\n",
    "bic_scores = []\n",
    "\n",
    "for n_c in n_clusters_list:\n",
    "    gmm = GaussianMixture(n_components=n_c,random_state=11) # fixing the random_state ensures results are same in all runs\n",
    "    gmm.fit(Signals)\n",
    "    bic_scores.append(gmm.bic(Signals))  # this calculates the BIC score for this GMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyS5z612ujhO",
    "outputId": "a8ab0ed5-6754-4f8d-f179-941818e9b793"
   },
   "outputs": [],
   "source": [
    "# Plot BIC scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_clusters_list, bic_scores, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('BIC score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdZNrna5ujhO"
   },
   "source": [
    "The BIC score shows an \"elbow\" \n",
    "\n",
    "### Try the clustering with n_clusters = 3, 5, 6 etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWiy_-3aujhO"
   },
   "source": [
    "## 1.8 Next Notebook: [XTEC with GMM](https://github.com/CAMM-UTK/acns-AI-tutorial/blob/main/Diffraction_ML/XTEC_with_GMM.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY9R6MWyujhO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
